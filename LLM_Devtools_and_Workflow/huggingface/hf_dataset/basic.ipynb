{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains ex, that teaches how to use HF datasets. This is an important skill for training purposes. We will be taking guide from ChatGPT for taking suggessions for training a BERT model on small dataset, which takes less resources.\n",
    "\n",
    "My quarry:\n",
    "`I want to use a small dataset to pretrain BERT, to do Q & A using huggingface datasets. Can you give me an example that will run small amout of CPU?`\n",
    "\n",
    "Our Goal: to use finetuning with a basic model BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script uses only a small portion of the SQuAD training data (e.g. 100 examples) for demonstration purposes. In real-world use, you would use the full dataset (or another dataset) and adjust the training parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers python-dotenv torch datasets transformers[torch] evaluate\n",
    "#%pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.48.3\n",
      "Datasets version: 3.3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Datasets version:\", datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87599/87599 [00:00<00:00, 541533.34 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:00<00:00, 529687.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Load a small subset of the SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "# For demonstration, use only the first 10 training examples\n",
    "small_train_dataset = dataset[\"train\"].select(range(10))\n",
    "small_eval_dataset = dataset[\"validation\"].select(range(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the tokenizer and model\n",
    "model_checkpoint = \"bert-base-uncased\"  # a small BERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 272.13 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 798.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 3. Preprocessing the data\n",
    "max_length = 128  # reduced max length for the context + question, to lower memory usage\n",
    "doc_stride = 64  # reduced stride for splitting up long documents\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    \n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    \n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # The corresponding example index\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set start and end positions to the CLS index.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(0)\n",
    "            tokenized_examples[\"end_positions\"].append(0)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "            \n",
    "            # Find the start and end token indices that span the answer.\n",
    "            token_start_index = 0\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= 0:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(offsets) - 1\n",
    "            while token_end_index >= 0 and offsets[token_end_index][1] >= max_length:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # If the answer is not fully inside the context, label it (0, 0)\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(0)\n",
    "                tokenized_examples[\"end_positions\"].append(0)\n",
    "            else:\n",
    "                # Otherwise find the token indices corresponding to the answer\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_position = token_start_index - 1\n",
    "\n",
    "                while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_position = token_end_index + 1\n",
    "\n",
    "                tokenized_examples[\"start_positions\"].append(start_position)\n",
    "                tokenized_examples[\"end_positions\"].append(end_position)\n",
    "    \n",
    "    return tokenized_examples\n",
    "\n",
    "# Preprocess the train and evaluation datasets\n",
    "train_dataset = small_train_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=small_train_dataset.column_names,\n",
    ")\n",
    "eval_dataset = small_eval_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=small_eval_dataset.column_names,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 4. Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,                   # more frequent evaluation on 10 steps (batches) for demonstration\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,  # small batch size for CPU\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,             # only 1 epoch for demonstration\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,                # more frequent logging on 5 steps (batches) for demonstration\n",
    "    save_steps=10,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.53k/4.53k [00:00<00:00, 15.0MB/s]\n",
      "Downloading extra modules: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.32k/3.32k [00:00<00:00, 11.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the load_metric function and define the metric\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "# 5. Define a metric (using squad metric for example)\n",
    "squad_metric = load_metric(\"squad\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    # In a real scenario, you would post-process the predictions to convert token indices to text\n",
    "    # For this small demo, we simply return an empty dictionary.\n",
    "    return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create the Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 01:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.093300</td>\n",
       "      <td>4.184247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.331100</td>\n",
       "      <td>3.834701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.778100</td>\n",
       "      <td>3.667915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=33, training_loss=3.4859717108986596, metrics={'train_runtime': 97.6499, 'train_samples_per_second': 0.338, 'train_steps_per_second': 0.338, 'total_flos': 2155698243072.0, 'train_loss': 3.4859717108986596, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 3.65807843208313, 'eval_runtime': 4.0287, 'eval_samples_per_second': 4.964, 'eval_steps_per_second': 4.964, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Evaluate the model (if desired)\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 9. Save the model\n",
    "trainer.save_model(\"./my_finetuned_qa_model\")        # save the model to disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Your Model and Tokenizer for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Define the path where your fine-tuned model is saved\n",
    "model_path = \"./my_finetuned_qa_model\"\n",
    "\n",
    "# Load the tokenizer and model from that directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "# Create a question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example context and question\n",
    "context = (\n",
    "    \"The fine-tuned BERT model was trained on a small subset of the SQuAD dataset. \"\n",
    "    \"It is now capable of answering questions based on the provided context.\"\n",
    ")\n",
    "question = \"What was the model trained on?\"\n",
    "\n",
    "# Use the QA pipeline to get an answer\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "print(\"Answer:\", result[\"answer\"])\n",
    "print(\"Score:\", result[\"score\"])\n",
    "print(\"Start index:\", result[\"start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
